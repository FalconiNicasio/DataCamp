{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to know PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Spark, anyway?\n",
    "\n",
    "- Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple *nodes*(think of each node as a separate computer). Splitting upo your data makes it easier to work wtih very large datasets because each node only works with a small amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark in Python\n",
    "\n",
    "- The first step in using Spark is connectig to a cluster.\n",
    "- Creating the connetion is as simple as creating an instance of the `SparkContext` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the SparkConext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=DataCampTutorial>\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "# make the connection\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = 'DataCampTutorial')\n",
    "\n",
    "# verify sparkcontext\n",
    "print(sc)\n",
    "\n",
    "# print spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataFrames\n",
    "\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD).\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a `SparkSession` object from your `SparkContext`.\n",
    "\n",
    "You can think of `SparkContext` as your **connection** to the cluster and\n",
    "`SparkSession` as your **interface** with that connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f433b732c50>\n"
     ]
    }
   ],
   "source": [
    "# make interface to work with\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# check if interface was loaded correctly\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "|2014|   11|  8|    1653|       -2|    1924|       -1|     AS| N323AS|   448|   SEA| LAX|     130|     954|  16|    53|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20|\n",
      "|2014|   10| 30|     811|       21|    1038|       29|     AS| N433AS|   608|   SEA| LAS|     127|     867|   8|    11|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|\n",
      "|2014|   10| 31|    1314|       89|    1544|      111|     AS| N713AS|   306|   SEA| SFO|     129|     679|  13|    14|\n",
      "|2014|    1| 29|    2009|        3|    2159|        9|     UA| N27205|  1458|   PDX| SFO|      90|     550|  20|     9|\n",
      "|2014|   12| 17|    2015|       50|    2150|       41|     AS| N626AS|   368|   SEA| SMF|      76|     605|  20|    15|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56|\n",
      "|2014|    6|  5|    1733|      -12|    1945|      -10|     OO| N215AG|  3488|   PDX| BUR|     111|     817|  17|    33|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/flights.csv\"\n",
    "\n",
    "# read data\n",
    "flights = spark.read.csv(file_path, header=True)\n",
    "\n",
    "#show data\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make table in SparkSession interface\n",
    "flights.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# check table was created\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are you query-ious?\n",
    "\n",
    "Now that we have created table in our `SparkSession` we can start making queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# get the first 10 rows of flights\n",
    "flights10 = spark.sql(query)\n",
    "\n",
    "flights10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandafy a Spark DataFrame\n",
    "\n",
    "Sometimes it makes sense to take the data you just queried and work wit it locally using `pandas`. Spark makes it easy with the `.toPandas()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  origin dest    N\n",
      "0    SEA  RNO    8\n",
      "1    SEA  DTW   98\n",
      "2    SEA  CLE    2\n",
      "3    SEA  LAX  450\n",
      "4    PDX  SEA  144\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "\n",
    "# run query\n",
    "flight_counts = spark.sql(query)\n",
    "\n",
    "# convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()\n",
    "\n",
    "# print head of pd_counts\n",
    "print(pd_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating columns\n",
    "\n",
    "The `.withColumn()` method allows you to perform column-wise operations on a Spark `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame called flights_df\n",
    "flights_df = spark.table(\"flights\") #note \"flights\" is a table in the SparkSession\n",
    "\n",
    "# Show the head of flights_df\n",
    "print(flights.show(5))\n",
    "\n",
    "# add duration_hrs column\n",
    "# *note column air_time contains duration of flight in minutes\n",
    "flights_df = flights_df.withColumn(\"duration_hrs\", flights_df.air_time / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "\n",
    "The `.filter()` method is similar to SQL's `WHERE` clause. The `.filter()` method takes a BOOLEAN expression or the `WHERE` clause of an SQL expression\n",
    "\n",
    "For example, the following two expressions will produce the same output:\n",
    "\n",
    "> flights.filter(flights.air_time > 120).show()\n",
    "<br>flights.filter(\"air_time > 120\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|              2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|               3.3|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20| 2.566666666666667|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|              3.05|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|               3.6|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56| 4.833333333333333|\n",
      "|2014|    9| 26|     610|       -5|    1523|       65|     US| N127UW|   616|   SEA| PHL|     293|    2378|   6|    10| 4.883333333333334|\n",
      "|2014|   12|  4|     954|       -6|    1348|      -17|     HA| N395HA|    29|   SEA| OGG|     333|    2640|   9|    54|              5.55|\n",
      "|2014|    6|  4|    1115|        0|    1346|       -3|     AS| N461AS|   488|   SEA| SAN|     133|    1050|  11|    15| 2.216666666666667|\n",
      "|2014|    6| 26|    2054|       -1|    2318|       -6|     B6| N590JB|   907|   SEA| ANC|     179|    1448|  20|    54|2.9833333333333334|\n",
      "|2014|    6|  7|    1823|       -7|    2112|      -28|     AS| N512AS|   815|   SEA| LIH|     335|    2701|  18|    23| 5.583333333333333|\n",
      "|2014|    4| 30|     801|        1|    1757|       90|     AS| N407AS|    18|   SEA| MCO|     342|    2554|   8|     1|               5.7|\n",
      "|2014|   11| 29|     905|      155|    1655|      170|     DL| N824DN|  1598|   SEA| ATL|     229|    2182|   9|     5| 3.816666666666667|\n",
      "|2014|    6|  2|    2222|        7|      55|       15|     AS| N402AS|    99|   SEA| ANC|     190|    1448|  22|    22|3.1666666666666665|\n",
      "|2014|   11| 15|    1034|       -6|    1414|      -26|     AS| N589AS|   794|   SEA| ABQ|     139|    1180|  10|    34| 2.316666666666667|\n",
      "|2014|   10| 20|    1328|       -1|    1949|        4|     UA| N68805|  1212|   SEA| IAH|     228|    1874|  13|    28|               3.8|\n",
      "|2014|   12| 16|    1500|        0|    1906|       19|     US| N662AW|   500|   SEA| PHX|     151|    1107|  15|     0|2.5166666666666666|\n",
      "|2014|   11| 19|    1319|       -6|    1821|      -14|     DL| N309US|  2164|   PDX| MSP|     169|    1426|  13|    19| 2.816666666666667|\n",
      "|2014|    5| 21|     515|        0|     757|        0|     US| N172US|   593|   SEA| PHX|     143|    1107|   5|    15|2.3833333333333333|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|              2.25|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|               3.3|\n",
      "|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA| PHX|     154|    1107|  11|    20| 2.566666666666667|\n",
      "|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA| ANC|     183|    1448|  23|    46|              3.05|\n",
      "|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA| MDW|     216|    1733|  10|    17|               3.6|\n",
      "|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA| BOS|     290|    2496|  21|    56| 4.833333333333333|\n",
      "|2014|    9| 26|     610|       -5|    1523|       65|     US| N127UW|   616|   SEA| PHL|     293|    2378|   6|    10| 4.883333333333334|\n",
      "|2014|   12|  4|     954|       -6|    1348|      -17|     HA| N395HA|    29|   SEA| OGG|     333|    2640|   9|    54|              5.55|\n",
      "|2014|    6|  4|    1115|        0|    1346|       -3|     AS| N461AS|   488|   SEA| SAN|     133|    1050|  11|    15| 2.216666666666667|\n",
      "|2014|    6| 26|    2054|       -1|    2318|       -6|     B6| N590JB|   907|   SEA| ANC|     179|    1448|  20|    54|2.9833333333333334|\n",
      "|2014|    6|  7|    1823|       -7|    2112|      -28|     AS| N512AS|   815|   SEA| LIH|     335|    2701|  18|    23| 5.583333333333333|\n",
      "|2014|    4| 30|     801|        1|    1757|       90|     AS| N407AS|    18|   SEA| MCO|     342|    2554|   8|     1|               5.7|\n",
      "|2014|   11| 29|     905|      155|    1655|      170|     DL| N824DN|  1598|   SEA| ATL|     229|    2182|   9|     5| 3.816666666666667|\n",
      "|2014|    6|  2|    2222|        7|      55|       15|     AS| N402AS|    99|   SEA| ANC|     190|    1448|  22|    22|3.1666666666666665|\n",
      "|2014|   11| 15|    1034|       -6|    1414|      -26|     AS| N589AS|   794|   SEA| ABQ|     139|    1180|  10|    34| 2.316666666666667|\n",
      "|2014|   10| 20|    1328|       -1|    1949|        4|     UA| N68805|  1212|   SEA| IAH|     228|    1874|  13|    28|               3.8|\n",
      "|2014|   12| 16|    1500|        0|    1906|       19|     US| N662AW|   500|   SEA| PHX|     151|    1107|  15|     0|2.5166666666666666|\n",
      "|2014|   11| 19|    1319|       -6|    1821|      -14|     DL| N309US|  2164|   PDX| MSP|     169|    1426|  13|    19| 2.816666666666667|\n",
      "|2014|    5| 21|     515|        0|     757|        0|     US| N172US|   593|   SEA| PHX|     143|    1107|   5|    15|2.3833333333333333|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# filter flights with SQL string\n",
    "long_flights1 = flights_df.filter(\"distance > 1000\")\n",
    "\n",
    "# filter flights with boolean statement\n",
    "long_flights2 = flights_df.filter(flights_df.distance > 1000)\n",
    "\n",
    "print(long_flights1.show())\n",
    "\n",
    "print(long_flights2.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting\n",
    "\n",
    "`.select()` method is Spark's variant of SQL's `SELECT` statement. The method takes multiple arguments - one for each column you want to select.Simi\n",
    "\n",
    "Similar to SQL, you can use `.alias()` and `.select()` methods to perform any column operation and return a transformed column with a new name.lights\n",
    "\n",
    "For example,\n",
    "> flights_df.select((flights_df.air_time / 60).(\"duration_hrs\"))\n",
    "\n",
    "Or maybe an easier way is to use `.selectExpr()` method\n",
    "> flights_df.selectExpr(\"air_time/60 as duration_hrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns tailnum, origin, and dest\n",
    "selected1 = flights_df.select(\"tailnum\", \"origin\", \"dest\")\n",
    "\n",
    "# select columns origin, dest, and carrier using df.colName syntax\n",
    "temp = flights.select(flights_df.origin, flights_df.dest, flights_df.carrier)\n",
    "\n",
    "# define first filter\n",
    "filterA = flights_df.origin == \"SEA\"\n",
    "\n",
    "# define second filter\n",
    "filterB = flights_df.dest ==\"PDX\"\n",
    "\n",
    "# filter the data\n",
    "selected2 = temp.filter(filterA).filter(filterB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define avg_speed\n",
    "# *note avg_speed is distance column divided by air_time column in hours\n",
    "avg_speed = (flights_df.distance / (flights_df.air_time / 60)).alias(\"avg_speed\")\n",
    "\n",
    "# select the correct columns\n",
    "speed1 = flights_df.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "\n",
    "# Create the same table using a SQL expression\n",
    "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating\n",
    "\n",
    "**GroupedData** are methods in python like `.min()`,`.max()`, and `count()`. These can be accessed in PySpark by calling `.groupBy()` method.\n",
    "\n",
    "For example\n",
    "> df.groupBy().min(\"col\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, year: string, month: string, day: string, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: string, origin: string, dest: string, air_time: string, distance: string, hour: string, minute: string, duration_hrs: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|          air_time|         distance|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|             10000|            10000|\n",
      "|   mean|152.88423173803525|        1208.1516|\n",
      "| stddev|  72.8656286392139|656.8599023464376|\n",
      "|    min|               100|             1009|\n",
      "|    max|                NA|              991|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.describe(\"air_time\", \"distance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(distance)|\n",
      "+-------------+\n",
      "|        106.0|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|max(air_time)|\n",
      "+-------------+\n",
      "|        409.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change distance and air_time colums to float data\n",
    "\n",
    "flights_df = flights_df.withColumn(\"distance\", flights_df.distance.cast(\"float\"))\n",
    "flights_df = flights_df.withColumn(\"air_time\", flights_df.air_time.cast(\"float\"))\n",
    "\n",
    "# find the shortest flight from PDX in terms of distance\n",
    "flights_df.filter(flights_df.origin == \"PDX\").groupBy().min(\"distance\").show()\n",
    "\n",
    "# find the longest flight from SEA in terms of duration\n",
    "flights_df.filter(flights_df.origin == \"SEA\").groupBy().max(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(air_time)|\n",
      "+------------------+\n",
      "|188.20689655172413|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "| sum(duration_hrs)|\n",
      "+------------------+\n",
      "|25289.600000000126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find avg duration of Delta flights that left from Seattle\n",
    "flights_df.filter(flights_df.carrier == \"DL\").filter(flights_df.origin == \"SEA\").groupBy().avg(\"air_time\").show()\n",
    "\n",
    "# find total of air_time in hours\n",
    "# *hint create new column called duration_hrs\n",
    "flights_df.withColumn(\"duration_hrs\", flights_df.air_time/60).groupBy().sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating\n",
    "\n",
    "You can pass one or more columns in your DataFrame to the `.groupBy()` method and aggregate the groups like how you would in SQL `GROUP BY` statement.\n",
    "\n",
    "Spark's **GroupedData** also has a `.agg()` method that allows you to access aggregate functions from `pyspark.sql.functions` submodule. This submodule contains useful functions for computing things like *standard deviation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|tailnum|count|\n",
      "+-------+-----+\n",
      "| N442AS|   38|\n",
      "| N102UW|    2|\n",
      "| N36472|    4|\n",
      "| N38451|    4|\n",
      "| N73283|    4|\n",
      "| N513UA|    2|\n",
      "| N954WN|    5|\n",
      "| N388DA|    3|\n",
      "| N567AA|    1|\n",
      "| N516UA|    2|\n",
      "| N927DN|    1|\n",
      "| N8322X|    1|\n",
      "| N466SW|    1|\n",
      "|  N6700|    1|\n",
      "| N607AS|   45|\n",
      "| N622SW|    4|\n",
      "| N584AS|   31|\n",
      "| N914WN|    4|\n",
      "| N654AW|    2|\n",
      "| N336NW|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+------------------+\n",
      "|origin|     avg(air_time)|\n",
      "+------+------------------+\n",
      "|   SEA| 160.4361496051259|\n",
      "|   PDX|137.11543248288737|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by tailnum\n",
    "by_plane = flights_df.groupBy(\"tailnum\")\n",
    "\n",
    "# get number of flights of each plane\n",
    "by_plane.count().show()\n",
    "\n",
    "# group by origin\n",
    "by_origin = flights_df.groupBy(\"origin\")\n",
    "\n",
    "# get average duration of flights from PDX and SEA\n",
    "by_origin.avg(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+\n",
      "|month|dest|      avg(dep_delay)|\n",
      "+-----+----+--------------------+\n",
      "|   11| TUS| -2.3333333333333335|\n",
      "|   11| ANC|   7.529411764705882|\n",
      "|    1| BUR|               -1.45|\n",
      "|    1| PDX| -5.6923076923076925|\n",
      "|    6| SBA|                -2.5|\n",
      "|    5| LAX|-0.15789473684210525|\n",
      "|   10| DTW|                 2.6|\n",
      "|    6| SIT|                -1.0|\n",
      "|   10| DFW|  18.176470588235293|\n",
      "|    3| FAI|                -2.2|\n",
      "|   10| SEA|                -0.8|\n",
      "|    2| TUS| -0.6666666666666666|\n",
      "|   12| OGG|  25.181818181818183|\n",
      "|    9| DFW|   4.066666666666666|\n",
      "|    5| EWR|               14.25|\n",
      "|    3| RDM|                -6.2|\n",
      "|    8| DCA|                 2.6|\n",
      "|    7| ATL|   4.675675675675675|\n",
      "|    4| JFK| 0.07142857142857142|\n",
      "|   10| SNA| -1.1333333333333333|\n",
      "+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+----+----------------------+\n",
      "|month|dest|stddev_samp(dep_delay)|\n",
      "+-----+----+----------------------+\n",
      "|   11| TUS|    3.0550504633038935|\n",
      "|   11| ANC|    18.604716401245316|\n",
      "|    1| BUR|     15.22627576540667|\n",
      "|    1| PDX|     5.677214918493858|\n",
      "|    6| SBA|     2.380476142847617|\n",
      "|    5| LAX|     13.36268698685904|\n",
      "|   10| DTW|     5.639148871948674|\n",
      "|    6| SIT|                   NaN|\n",
      "|   10| DFW|     45.53019017606675|\n",
      "|    3| FAI|    3.1144823004794873|\n",
      "|   10| SEA|     18.70523227029577|\n",
      "|    2| TUS|    14.468356276140469|\n",
      "|   12| OGG|     82.64480404939947|\n",
      "|    9| DFW|    21.728629347782924|\n",
      "|    5| EWR|     42.41595968929191|\n",
      "|    3| RDM|      2.16794833886788|\n",
      "|    8| DCA|     9.946523680831074|\n",
      "|    7| ATL|    22.767001039582183|\n",
      "|    4| JFK|     8.156774303176903|\n",
      "|   10| SNA|    13.726234873756304|\n",
      "+-----+----+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change dep_delay column to float\n",
    "flights_df = flights_df.withColumn(\"dep_delay\", flights_df.dep_delay.cast(\"float\"))\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# group by month and dest\n",
    "by_month_dest = flights_df.groupBy(\"month\", \"dest\")\n",
    "\n",
    "# get average departure delay by month and destination\n",
    "by_month_dest.avg(\"dep_delay\").show()\n",
    "\n",
    "# get standard deviation of departure delays\n",
    "by_month_dest.agg(F.stddev(\"dep_delay\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining\n",
    "\n",
    "In Pyspark, joins are performed using the DataFrame method `.join()`. This method takes three arguments:\n",
    "<br>-The first is the second DataFrame that you want to join with the first one\n",
    "<br>-Then the `on` statement to name the key of which the colum(s) share to be joined on\n",
    "<br>-Third is `how` statement that specifies the kind of join to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|faa|                name|       lat|        lon| alt| tz|dst|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport| 41.431912|-74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|31.0744722|-81.4277778|  11| -4|  A|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data to spark session\n",
    "file_path = \"../data/airports.csv\"\n",
    "airports = spark.read.csv(file_path, header = True)\n",
    "\n",
    "# load DataFrame as a table into catalog\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# check table was created\n",
    "spark.catalog.listTables()\n",
    "\n",
    "# switching to flight and not flights_df, realized creating flights_df was redundant\n",
    "\n",
    "# examine data\n",
    "airports.describe()\n",
    "\n",
    "airports.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+---+---+---+\n",
      "|dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute|                name|      lat|        lon|alt| tz|dst|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+---+---+---+\n",
      "| LAX|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA|     132|     954|   6|    58|    Los Angeles Intl|33.942536|-118.408075|126| -8|  A|\n",
      "| HNL|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA|     360|    2677|  10|    40|       Honolulu Intl|21.318681|-157.922428| 13|-10|  N|\n",
      "| SFO|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA|     111|     679|  14|    43|  San Francisco Intl|37.618972|-122.374889| 13| -8|  A|\n",
      "| SJC|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX|      83|     569|  17|     5|Norman Y Mineta S...|  37.3626|-121.929022| 62| -8|  A|\n",
      "| BUR|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA|     127|     937|   7|    54|            Bob Hope|34.200667|-118.358667|778| -8|  A|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename faa column\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
    "\n",
    "# join the DataFrames\n",
    "flights_with_airports = flights.join(airports, on = \"dest\", how = \"leftouter\")\n",
    "\n",
    "flights_with_airports.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with machine learning pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Pipelines\n",
    "\n",
    "At the core of the `pysark.ml` module are the `Transformer` and `Estimator` classes. Almost every other class in the module behaves similarly to these two basic classes.\n",
    "\n",
    "`Transformer` classes have a `.transform()` method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class `Bucketizer` to create discrete bins from a continuous feature or the class `PCA` to reduce the dimensionality of your dataset using principal component analysis.\n",
    "\n",
    "`Estimator` classes all implement a `.fit()` method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a `StringIndexerModel` for including categorical data saved as strings in your models, or a `RandomForestModel` that uses the random forest algorithm for classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/planes.csv\"\n",
    "planes = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Rename year column\n",
    "planes = planes.withColumnRenamed(\"year\", \"plane_year\")\n",
    "\n",
    "# join the DataFrames\n",
    "model_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types\n",
    "\n",
    "It's important to know that SPark only handles numeric data. That means all of the columns in your DataFrame must be either integers or decimals (called 'doubles' in Spark).\n",
    "\n",
    "When we imported our data, we let Spark guess what kind of information each column held. Unfortunately, Spark doesn't always guess right and you can see that some of the columns in our DataFrame aree strings containing numbers as opposed to actual numeric values.\n",
    "\n",
    "To remedy this, you can use the `.cast()` method in combination with the `.withColumn()` method. It's important to note that `.cast()` works on columns, while `.withColumn()` works on DataFrames.\n",
    "\n",
    "The only argument you need to pass to `.cast()` method is the kind of value you want to create, in string form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the columns to integers\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the coluimn plane_age\n",
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create is_late\n",
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n",
    "\n",
    "# convert to an integer\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))\n",
    "\n",
    "# remove missing values\n",
    "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings and factors\n",
    "\n",
    "`pyspark.ml.features` submodule has functions that can handle non numerical data for modeling\n",
    "\n",
    "Steps for Encoding your categorical feature\n",
    "- Create a `StringIndexer`\n",
    "    - Members of this class are `Estimator`s that take a DataFrame with a column of strings and maps each unqique string to a number.\n",
    "    - The `Estimator` then returns a `Transformer` that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.\n",
    "- Second step is to encode this numeric column using `OneHotEncoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator \n",
    "\n",
    "# create a StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n",
    "\n",
    "# create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoderEstimator(inputCols=[\"carrier_index\"], outputCols=[\"carrier_fact\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a StringIndexer\n",
    "dest_indexer = StringIndexer(inputCol=\"dest\", outputCol=\"dest_index\")\n",
    "\n",
    "# create a OneHotEncoder\n",
    "dest_encoder = OneHotEncoderEstimator(inputCols=[\"dest_index\"], outputCols=[\"dest_fact\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble a vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\",\n",
    "                                          \"plane_age\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# make the pipeline\n",
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, \n",
    "                                carr_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test vs Train\n",
    "\n",
    "Its good practice to now split the data into a training and testing set in order to see your model's performance on unseen data. It's important to make sure you perform all of the transformations **BEFROE** splitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the data\n",
    "piped_data = flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "training, test = piped_data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is logistic regression?\n",
    "\n",
    "This model is very similar to a linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# create a logisticregression estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "This model splits the data into partitions and is a good estimate of the model's error on the test set or unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# create a BinaryClassficiationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "# create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the crossvalidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "                        estimatorParamMaps=grid,\n",
    "                        evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid = LogisticRegression_b782cecd3956, numClasses = 2, numFeatures = 81\n"
     ]
    }
   ],
   "source": [
    "# call lr.fit()\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "# print best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating binary classifiers\n",
    "\n",
    "*AUC*, or area under the curve will be used as the metric to evaluate binary classification algorithms. The closer *AUC* is to one(1), the better the model is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6942929666404072\n"
     ]
    }
   ],
   "source": [
    "# use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "**The next steps are learning how to create large scale Spark clusters and manage and submit jobs so that you can use models in the real world.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

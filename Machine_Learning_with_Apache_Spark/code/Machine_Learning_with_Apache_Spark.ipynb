{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Spark is a framework for working with Big Data. In this chapter you'll cover some background about SPark and Machine Learning. You'l then find out how to connect to Spark using Python and load CSV data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning & Spark\n",
    "\n",
    "Data in RAM\n",
    "- The performance on a ML model depends on data\n",
    "- If the data can't fit entirely in RAM then it will start to page data between RAM and Disk and the bigger the data the more time the computer spends waiting for data.\n",
    "- One option to solve is to distribute the data amoung clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Spark\n",
    "\n",
    "Spark is currently the most popular technology for processing large quantities of data. Not only is it able to handle enormous data volumes, but it does so very efficiently too! Also, unlike some other distributed computing technologies, developing with Spark is a pleasure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in a Spark Cluster\n",
    "Spark is a distributed computing platform. It achieves efficiency by distributing data and computation across a cluster of computers.\n",
    "A Spark cluster consists of a number of hardware and software components which work together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Spark\n",
    "Languages for interacting with Spark.\n",
    "- Java - low-level, compiled\n",
    "- Scala, Python, and R - high-level with interactive REPL (read.eval.print.loop)\n",
    "\n",
    "Importing pyspark\n",
    "- import `pyspark`\n",
    "\n",
    "Sub-modules\n",
    "- Structured Data - `pyspark.sql`\n",
    "- Streaming Data - `pyspark.streaming`\n",
    "- Machine Learning - `pyspark.mllib` (deprecated) and `pyspark.ml`\n",
    "\n",
    "Spark URL\n",
    "- **Remote Cluster** using Spark URL - spark://<IP address | DNS name>:<\\port>\n",
    "    - *example* spark://13.59.151.161:7077\n",
    "- **Local Cluster**\n",
    "    - `local` - only 1 core;\n",
    "    - ` local[4]` - 4 cores;\n",
    "    - `locall[*]` - all available cores.\n",
    "\n",
    "Creating a SparkSession\n",
    "- connect to spark by using:\n",
    "<br>```from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('first_spark_app').getOrCreate()```\n",
    "\n",
    "- close connection to spark\n",
    "<br>`spark.stop()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of Spark master\n",
    "The following are all examples of valid ways to specify the location of a Spark Cluster:\n",
    "- spark://13.59.151.161:7077\n",
    "- spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077\n",
    "- local\n",
    "- local[4]\n",
    "- local[*]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "# import the PySpark module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# what version of Spark?\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "`DataFrame` for tabular data.\n",
    "\n",
    "Reading data from CSV\n",
    "<br>&emsp;`cars = spark.read.csv('cars.csv', header=True)`\n",
    "\n",
    "- Optional arguments:\n",
    "    - `header` - is first row a header? (defaule:`False`)\n",
    "    - `sep` - field separator (default: a comma `','`)\n",
    "    - `schema` - explicit column data types\n",
    "    - `inferSchema` - deduce column data types from data?\n",
    "    - `nullValue` - placeholder for missing data\n",
    "    \n",
    "Peek at the data\n",
    "<br>&emsp;&emsp;`cars.show(5)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading flights data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 50000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mon', 'int'),\n",
       " ('dom', 'int'),\n",
       " ('dow', 'int'),\n",
       " ('carrier', 'string'),\n",
       " ('flight', 'int'),\n",
       " ('org', 'string'),\n",
       " ('mile', 'int'),\n",
       " ('depart', 'double'),\n",
       " ('duration', 'int'),\n",
       " ('delay', 'int')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from CSV file\n",
    "flights = spark.read.csv('../data/flights.csv',\n",
    "                        sep=',',\n",
    "                        header=True,\n",
    "                        inferSchema=True,\n",
    "                        nullValue='NA')\n",
    "\n",
    "# get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# view the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# check column data types\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SMS spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# load data from a delimited file\n",
    "sms = spark.read.csv(\"../data/sms.csv\", sep=';', header=False, schema=schema)\n",
    "\n",
    "# print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Now that you are familiar with getting data into Spark, you'll move onto buiilding two types of classification model: Decision Trees and Logistic Regression. You'll also find out about a few approaches to data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Dropping Colunns\n",
    "- Drop the columns you don't want\n",
    "<br>&emsp;`cars = cars.drop('maker', 'model')`\n",
    "\n",
    "- Select columns you want to retain\n",
    "<br>&emsp;`cars = cars.select('origin', 'type', 'cyl', 'size', 'weight', 'length', 'rpm', 'consumption')`\n",
    "\n",
    "Filtering out missing data\n",
    "- How many missing values?\n",
    "<br>&emsp;`cars.filter('cyl IS NULL').count()`\n",
    "\n",
    "- Drop records with missing values in the `cylinders` column\n",
    "<br>&emsp;`cars = cars.filter('cyl IS NOT NULL')`\n",
    "\n",
    "- Drop records with missing values in **any** column\n",
    "<br>&emsp;`cars = cars.dropna()`\n",
    "\n",
    "Mutating columns\n",
    "<br>&emsp;`from pyspark.sql.functions import round`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`cars = cars.withColumn('mass', round(cars.weight / 2.205, 0)`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`cars.withColumn('length', round(cars.length * 0.0254, 3))`\n",
    "\n",
    "Indexing categorical data\n",
    "<br>&emsp;`From pyspark.ml.feature import StringIndexer`\n",
    "<br>&emsp;\n",
    "<br>&emsp; `indexer = StringIndexer(inputCol='type', outputCol='type_idx')`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`indexer = indexer.fit(cars)`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`cars = indexer.transform(cars)`\n",
    "\n",
    "Assembling columns\n",
    "- consolidate input columns into a single column\n",
    "- ML algorithms in Spark operate on a single vector\n",
    "<br>&emsp;`from pyspark.ml.feature import VectorAssembler`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`assembler = VectorAssembler(inputCols=['cyl','size'], outputCol='features')`\n",
    "<br>&emsp;`assembler.transform(cars)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n"
     ]
    }
   ],
   "source": [
    "# remove the 'flight' column\n",
    "flights = flights.drop('flight')\n",
    "\n",
    "# number of records with missing 'delay' values\n",
    "flights.filter('delay IS NULL').count()\n",
    "\n",
    "# remove records with missing 'delay' values\n",
    "flights = flights.filter('delay IS NOT NULL')\n",
    "\n",
    "# remove records with missing values in any column and get the number of remaining rows\n",
    "flights = flights.dropna()\n",
    "print(flights.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# convert 'mile' to 'km' and drop 'mile' column\n",
    "flights_km = flights.withColumn('km', round(flights.mile * 1.60934)).drop('mile')\n",
    "\n",
    "# create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# indexer identifies catgories in the data\n",
    "indexer_model = indexer.fit(flights_km)\n",
    "\n",
    "# indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights_km)\n",
    "\n",
    "# repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |30   |\n",
      "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |-8   |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|-5   |\n",
      "|[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]   |2    |\n",
      "|[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0] |54   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_indexed)\n",
    "\n",
    "# check the resulting columns\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Recursive partitioning\n",
    "\n",
    "Stopping criteria examples (stop splitting)\n",
    "- number of records of a node fall below a threshold\n",
    "- purity of a node is above a threshold\n",
    "\n",
    "Split train/test\n",
    "- `train, test = cars.randomSplit([0.8, 0.2], seed=23)`\n",
    "\n",
    "Build a decision tree\n",
    "- `from pyspark.ml.classification import DecisionTreeClassifier`\n",
    "- `tree = DecisionTreeClassifier()`\n",
    "- `tree_model = tree.fit(train)`\n",
    "- `prediction = tree.transform(test)`\n",
    "\n",
    "Confusion matrix\n",
    "- A confusion matrix is a table which describes performance of a model on testing data.\n",
    "- `prediction.groupBy(\"label\", \"prediction\").count().show()`\n",
    "- *note Accuracy = (TN + TP)/(TN + TP + FN + FP) - proportion of correct predictions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980732423121092\n"
     ]
    }
   ],
   "source": [
    "# split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# check that training set has around 80% of recods\n",
    "training_ratio = flights_train.count()/flights.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------------------------------+\n",
      "|label|prediction|probability                            |\n",
      "+-----+----------+---------------------------------------+\n",
      "|1    |1.0       |[0.493801652892562,0.506198347107438]  |\n",
      "|1    |1.0       |[0.3550259700580507,0.6449740299419493]|\n",
      "|1    |1.0       |[0.3550259700580507,0.6449740299419493]|\n",
      "|1    |1.0       |[0.3550259700580507,0.6449740299419493]|\n",
      "|1    |1.0       |[0.3550259700580507,0.6449740299419493]|\n",
      "+-----+----------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|            features|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|[0.0,22.0,2.0,0.0...|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|[2.0,20.0,4.0,0.0...|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|[9.0,13.0,1.0,1.0...|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|[5.0,2.0,1.0,0.0,...|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|[7.0,2.0,6.0,1.0,...|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_assembled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1218|\n",
      "|    0|       0.0| 2424|\n",
      "|    1|       1.0| 3606|\n",
      "|    0|       1.0| 2247|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0.6350710900473934\n"
     ]
    }
   ],
   "source": [
    "# create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# accuracy measure the propeortion of correct predictions\n",
    "accuracy = (TN + TP)/(TN + TP + FN + FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- Uses a logistic function to measure a binary target (0,1 : true, false)\n",
    "- X-axis is a linear combination of predictor variables\n",
    "- y-axis output of the model\n",
    "- the model dreives coefficients for each numerical predictor\n",
    "    - can shift the curve to the right or left\n",
    "    - can make the transition more gradual or steep\n",
    "    \n",
    "Build a Logistic Regression model\n",
    "- `from pyspark.ml.classification import LogisticRegression`\n",
    "- `logistic = LogisticRegression()`\n",
    "- `logistic_model = logistic.fit(train)`\n",
    "- `prediction = logistic_model.transform(test)`\n",
    "\n",
    "Precision and recall\n",
    "- build confusion matrix\n",
    "- precision = TP / (TP + FP) *note proportion of positive predictions*\n",
    "- recall = TP / (TP + FN) *note proportion of positive targets that are correctly predicted*\n",
    "\n",
    "Weighted metrics\n",
    "- `from pyspark.ml.evaluation import MulticlassClassificationEvaluator`\n",
    "- `evaluator = MulticlassClassificationEvaluator()`\n",
    "- `evaluator.evaluate(prediction, {evaluator.metricName: 'weightedPrecision'})`\n",
    "    - Other metrics: `weightedRecall`, `accuracy`, `f1`\n",
    "    \n",
    "ROC and AUC\n",
    "ROC = \"Receiver Operating Characteristic\"\n",
    "- TP versus FP\n",
    "- threshold = 0 (top right)\n",
    "- threshdol = 1 (bottom left)\n",
    "AUC = \"Area under the curve\"\n",
    "- ideally AUC = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1666|\n",
      "|    0|       0.0| 2636|\n",
      "|    1|       1.0| 3169|\n",
      "|    0|       1.0| 2024|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_num = flights_assembled.select('mon', 'depart', 'duration', 'features', 'label')\n",
    "flights_num_train, flights_num_test = flights_num.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# create a classifier object and train on the training data\n",
    "logistic = LogisticRegression().fit(flights_num_train)\n",
    "\n",
    "# create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_num_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.61\n",
      "recall = 0.66\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction,\n",
    "                                              {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Text into Tables\n",
    "\n",
    "Removing punctuation\n",
    "- `from pyspark.sql.functions import regexp_replace`\n",
    "- `REGEX = '[,\\\\-]'`\n",
    "- `books = books.withColumn('text', regexp_replace(books.text, REGEX, ' '))`\n",
    "\n",
    "Text to tokens\n",
    "- `from pyspark.ml.feature import Tokenizer`\n",
    "- `books = Tokenizer(inputCol=\"text\", outputCol=\"tokens\").transform(books)`\n",
    "\n",
    "What are stop words\"\n",
    "- `from pyspark.ml.feature import StopWordsRemover`\n",
    "- `stopwords = StopWordsRemover()`\n",
    "- `stopwords.getStopWords()`\n",
    "\n",
    "Removing stop words\n",
    "- `stopwords = stopwords.setInputCol('tokens').setOutputCol('words')`\n",
    "- `books = stopwords.transform(books)`\n",
    "\n",
    "Feature hashing\n",
    "- `from pyspark.ml.feature import HashingTF`\n",
    "- `hasher = HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=32)`\n",
    "- `books = hasher.transform(books)`\n",
    "\n",
    "Dealing with common words\n",
    "- IDF = inverse documents frequency\n",
    "- `from pyspark.ml feature import IDF`\n",
    "- `books = IDF(inputCol=\"hash\", outputCol=\"features\").fit(books).transform(books)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation, numbers and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|id |text                              |label|words                                     |\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# remove punctuation\n",
    "sms = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "sms = sms.withColumn('text', regexp_replace(sms.text, '[0-9]', ' '))\n",
    "\n",
    "# merge multiple spaces\n",
    "sms = sms.withColumn('text', regexp_replace(sms.text, ' +', ' '))\n",
    "\n",
    "# split the text into words\n",
    "sms = Tokenizer(inputCol='text', outputCol='words').transform(sms)\n",
    "\n",
    "sms.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words and hashing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|terms                           |features                                                                                            |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]   |(1024,[138,344,378,1006],[2.2391682769656747,2.892706319430574,3.684405173719015,4.244020961654438])|\n",
      "|[dont, worry, guess, busy]      |(1024,[53,233,329,858],[4.618714411095849,3.557143394108088,4.618714411095849,4.937168142214383])   |\n",
      "|[call, freephone]               |(1024,[138,396],[2.2391682769656747,3.3843005812686773])                                            |\n",
      "|[win, cash, prize, prize, worth]|(1024,[31,69,387,428],[3.7897656893768414,7.284881949239966,4.4671645129686475,3.898659777615979])  |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# remove stop words\n",
    "sms = StopWordsRemover(inputCol=\"words\", outputCol=\"terms\").transform(sms)\n",
    "\n",
    "# apply the hashing trick\n",
    "sms = HashingTF(inputCol=\"terms\", outputCol=\"hash\", numFeatures=1024).transform(sms)\n",
    "\n",
    "# convert hashed symbols to TF-IDF\n",
    "sms = IDF(inputCol=\"hash\", outputCol=\"features\").fit(sms).transform(sms)\n",
    "\n",
    "sms.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a spam classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1666|\n",
      "|    0|       0.0| 2636|\n",
      "|    1|       1.0| 3169|\n",
      "|    0|       1.0| 2024|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing sets\n",
    "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# fit a logistic regresssion model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "predictions = logistic.transform(sms_test)\n",
    "\n",
    "# create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Next you'll learn to create Linear Regression models. You'll also find out how to augment your data by engineering new predictors as well as a robiust approach to selecting ony the most relevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Dummy variables\n",
    "- Each categorical level becomes a column\n",
    "- Binary values indicate the presence (1) or absence (0) of the corresponding level\n",
    "- Sparse reprersentation: store column index and value\n",
    "\n",
    "One-hot encoding\n",
    "- `from pyspark.ml.feature import OneHotEncoderEstimator`\n",
    "- `onehot = OneHotEncoderEstimator(imputCols=['type_idx'], outputCols=['type_dummy'])`\n",
    "- `onehot = onehot.fit(cars)`\n",
    "- `onehot.categorySizes`\n",
    "- `cars = onehot.transform(cars)`\n",
    "- `cars.select('type, 'type_idx', 'type_dummy').distinct().sort('type_idx').show()`\n",
    "\n",
    "Dense versus sparse\n",
    "- `from pyspark.mllib.linalg import DenseVector, SparseVector`\n",
    "- Store this vector:[1, 0, 0, 0, 0, 7, 0, 0])\n",
    "- `DenseVector([1, 0, 0, 0, 0, 7, 0, 0])`\n",
    "    - `out:` DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0])\n",
    "- `SparseVector(8, [0, 5], [1, 7])`\n",
    "    - `out:` SparseVector(8, {0: 1.0, 5: 7.0})\n",
    "- SparseVector(length of the vector, index of non-zero values, non-zero values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset flights dataframe\n",
    "flights = spark.read.csv('../data/flights.csv',\n",
    "                        sep=',',\n",
    "                        header=True,\n",
    "                        inferSchema=True,\n",
    "                        nullValue='NA')\n",
    "\n",
    "flights = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights).transform(flights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding flight origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SJC|    4.0|(7,[4],[1.0])|\n",
      "|SMF|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights = onehot.transform(flights)\n",
    "\n",
    "# check the results\n",
    "flights.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enocidng shirt sizes\n",
    "You have data for a consignment of t-shirts. The data includes th size of the shirt, which is given as either S,M, L, or XL.\n",
    "\n",
    "Here Are the counts for the different sizes.\n",
    "\n",
    "`+----+-----+\n",
    "|size|count|\n",
    "+----+-----+\n",
    "|   S|    8|\n",
    "|   M|   15|\n",
    "|   L|   20|\n",
    "|  XL|    7|\n",
    "+----+-----+`\n",
    "\n",
    "The sizes are first converted to an index using `StringIndexer` and then one-hot encoded using `OneHotEncorderEstimator`\n",
    "\n",
    "Output:\n",
    "\n",
    "`+---+-------+-------------+\n",
    "|size|   idx|    idx_dummy|\n",
    "+---+-------+-------------+\n",
    "|  S|    2.0|(3,[2],[1.0])|\n",
    "|  M|    1.0|(3,[1],[1.0])|\n",
    "|  L|    0.0|(3,[0],[1.0])|\n",
    "| XL|    3.0|(3,[3],  [] )|\n",
    "+---+-------+-------------+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "Mininmizing Loss functiopn\n",
    "- MSE = \"Mean Squared Error\"\n",
    "\n",
    "Assemble predictors\n",
    "- Predict `consumptiopn` using `mass`,`cyl`,and `type_dummy`.\n",
    "- Consolidate predictors into a single column called `features`\n",
    "\n",
    "Build regression model\n",
    "- `from pyspark.ml.regression import LinearRegression`\n",
    "- `regression = LinearRegression(labelCol='consumption')`\n",
    "- `regression = regression.fit(cars_train)`\n",
    "- `predictions = regression.transform(cars_test)`\n",
    "\n",
    "Calculate RMSE\n",
    "- `from pyspark.ml.evaluation import RegressionEvaluator`\n",
    "- `RegressionEvaluator(labelCol='consumption').evaluate(predictions)`\n",
    "- `RegressionEvaluator` can also calculate the following metrics\n",
    "    - `mae` (Mean Absolute Error)\n",
    "    - `r2` (R^2)\n",
    "    - `mse` (Mean Squared Error)\n",
    "\n",
    "Examine intercept\n",
    "- `regression.intercept`\n",
    "- This is the fuel consumption in the (hypothetical) case that:\n",
    "    - `mass` = 0\n",
    "    - `cyl` = 0\n",
    "\n",
    "Examine Coefficients\n",
    "- `regression.coefficients`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flights = flights.withColumn('km', flights.mile * 1.60934).drop('mile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['km'], outputCol='features')\n",
    "flights = assembler.transform(flights)\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+----------+------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    org_dummy|        km|    features|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+----------+------------+\n",
      "| 11| 20|  6|     US|    19|JFK|  9.48|     351| null|    2.0|(7,[2],[1.0])|3464.90902|[3464.90902]|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30|    0.0|(7,[0],[1.0])| 508.55144| [508.55144]|\n",
      "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8|    1.0|(7,[1],[1.0])| 542.34758| [542.34758]|\n",
      "|  9| 13|  1|     AA|   419|ORD| 10.33|     195|   -5|    0.0|(7,[0],[1.0])|1989.14424|[1989.14424]|\n",
      "|  4|  2|  5|     AA|   325|ORD|  8.92|      65| null|    0.0|(7,[0],[1.0])| 415.20972| [415.20972]|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Just distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|385     |359.2696656028616 |\n",
      "|135     |149.93838859748362|\n",
      "|200     |224.09938202172748|\n",
      "|64      |72.97656947741446 |\n",
      "|259     |269.1561432154389 |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.039835602281954"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.35943736789506\n",
      "[0.07566768380408988]\n",
      "792.9408828654666\n"
     ]
    }
   ],
   "source": [
    "# intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# average minutes per km\n",
    "minutes_per_km = coefs[0]\n",
    "\n",
    "# average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Adding origin airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.drop('features')\n",
    "\n",
    "# add org_dummy to features\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "flights = assembler.transform(flights)\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.124011989821785"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting coefficients\n",
    "\n",
    "The values for km and org_dummy have been assembled into features, which has eight columns with sparse representation. Column indices in features are as follows:\n",
    "\n",
    "- 0 — km\n",
    "- 1 — ORD\n",
    "- 2 — SFO\n",
    "- 3 — JFK\n",
    "- 4 — LGA\n",
    "- 5 — SMF\n",
    "- 6 — SJC and\n",
    "- 7 — TUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807.3233680861835\n",
      "15.862995338536262\n",
      "68.52994024026248\n",
      "62.568008835633506\n"
     ]
    }
   ],
   "source": [
    "# average speed in km per hour\n",
    "avg_speed_hour = 60 / regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing & Engineering\n",
    "Bucketing\n",
    "- Binning values\n",
    "- `from pyspark.ml.feature import Bucketizer`\n",
    "- `bucketizer = Bucketizer(splits=[3500, 4500, 6000, 6500], inputCol=\"rpm\", outputCol= \"rpm_bin\")\n",
    "- `cars = bucketizer.transform(cars)`\n",
    "- onehotencode before applying to regression model\n",
    "\n",
    "More feature engineering\n",
    "- Operations on a single columns:\n",
    "    - `log()`\n",
    "    - `sqrt()`\n",
    "    - `pow()`\n",
    "- Operations on two columns:\n",
    "    - product\n",
    "    - ratio\n",
    "    \n",
    "Feature engineering examples:\n",
    "- Mass & Height to BMI\n",
    "    - potentially, BMI can be a more powerful predictor than mass or height in isolation\n",
    "- Engineering density\n",
    "    - cars = cars.withColumn('density_line', cars.mass / cars.length)    # Linear density\n",
    "    - cars = cars.withColumn('density_quad', cars.mass / cars.length**2) # Area density\n",
    "    - cars = cars.withColumn('density_cube', cars.mass / cars.length**3) # Volume density\n",
    "<br> *note - powerful new features are often discovered through trial and error*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing departure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "|  9.48|          3.0|\n",
      "| 16.33|          5.0|\n",
      "|  6.17|          2.0|\n",
      "| 10.33|          3.0|\n",
      "|  8.92|          2.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "|  9.48|          3.0|(7,[3],[1.0])|\n",
      "| 16.33|          5.0|(7,[5],[1.0])|\n",
      "|  6.17|          2.0|(7,[2],[1.0])|\n",
      "| 10.33|          3.0|(7,[3],[1.0])|\n",
      "|  8.92|          2.0|(7,[2],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0.00, 03.00, 06.00, 09.00, 12.00, 15.00, 18.00, 21.00, 24.00], \n",
    "                     inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# bucket the depatrue times\n",
    "flights = buckets.transform(flights)\n",
    "flights.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# create a one-hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed deptature times\n",
    "flights = onehot.fit(flights).transform(flights)\n",
    "flights.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Adding departure time\n",
    "`depart_dummy` index 8 to 14\n",
    "- 8 — 00:00 - 03:00\n",
    "- 9 — 03:00 - 06:00\n",
    "- 10 — 06:00 - 09:00\n",
    "- 11 — 09:00 - 12:00\n",
    "- 12 — 12:00 - 15:00\n",
    "- 13 — 15:00 - 18:00\n",
    "- 14 — 18:00 - 21:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset features columns to include depart_dummy\n",
    "flights = flights.drop('features')\n",
    "# create vectorassembler\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'depart_dummy'],\n",
    "                            outputCol='features')\n",
    "# create features column\n",
    "flights = assembler.transform(flights)\n",
    "# split data\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=13)\n",
    "# create regression model and fit on train set\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "# make predictions on test set\n",
    "predictions = regression.transform(flights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.483676375513884\n",
      "-4.121344067599443\n",
      "47.572817783321035\n"
     ]
    }
   ],
   "source": [
    "# find the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
    "avg_eve_ogg = regression.intercept\n",
    "print(avg_eve_ogg)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\n",
    "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
    "print(avg_night_ogg)\n",
    "\n",
    "# Average minutes on ground at JFK for flights depating between 00:00 and 03:00\n",
    "avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[8]\n",
    "print(avg_night_jfk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Lasso\n",
    "- absolute value of the coefficients\n",
    "- can force coefficients to zero\n",
    "- `lasso = LinearRegression(labelCol='consumption', elasticNetParam=1, regParam=0.1)`\n",
    "Ridge\n",
    "- square of the coefficients\n",
    "- can force coefficients close to zero\n",
    "- `ridge = LinearRegression(labelCol='consumption', elasticNetParam=0, regParam=0.1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: More features!\n",
    "These are the feature syou'll include in the next model:\n",
    "- `km`\n",
    "- `org` (origin airport, one-hot encoded, 8 levels)\n",
    "- `depart` (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels)\n",
    "- `dow` (departure day of week, one-hot encoded, 7 levels)\n",
    "- `mon` (departure month, one-hot encoded, 12 levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.drop('features')\n",
    "\n",
    "onehot = OneHotEncoderEstimator(inputCols=['dow', 'mon'], outputCols = ['dow_dummy', 'mon_dummy'])\n",
    "flights = onehot.fit(flights).transform(flights)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'depart_dummy', 'dow_dummy', 'mon_dummy'],\n",
    "                           outputCol='features')\n",
    "flights = assembler.transform(flights)\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 10.724055996754982\n",
      "[0.07443672229195626,27.074222675233724,20.077136393625985,51.76577422204704,45.645021378984076,17.45535293535999,14.979185825703254,17.06442064280308,-15.113860995892185,1.6427071994710414,4.056829859905945,6.874198429259234,4.632713855387798,8.815481970316627,8.730109378588802,0.35363690972964,0.06173617510577866,-0.16088770207113512,0.19148688860628943,0.20333490710532326,0.12115889895844358,-2.1856825118460708,-2.288796502807716,-2.0823432825081207,-3.666577161918417,-4.156684025422529,-4.377352065170049,-4.528997974139636,-4.284372374700983,-3.985546221334758,-2.9283459130415714,-0.7856999249283695]\n"
     ]
    }
   ],
   "source": [
    "# fit linear regression model to training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# make predictions on testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Regularisation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lasso test RSME is 11.70447567404799\n",
      "[0.07352732182283632,5.363611378756951,0.0,29.02745297158733,21.883182046066523,0.0,-2.3369548237782367,0.0,0.0,0.0,0.0,0.0,0.0,1.1715298130259522,1.1899197632965113,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Number of coefficients equal to 0: 25\n"
     ]
    }
   ],
   "source": [
    "# fit Lasso model ((α = 1)) to training data\n",
    "lasso = LinearRegression(labelCol='duration', elasticNetParam=1, regParam=1).fit(flights_train)\n",
    "\n",
    "# calculate the RMSE on testing data\n",
    "lasso_rmse = RegressionEvaluator(labelCol='duration').evaluate(lasso.transform(flights_test))\n",
    "print(\"The lasso test RSME is\", lasso_rmse)\n",
    "\n",
    "# look at the model coefficients\n",
    "lasso_coeffs = lasso.coefficients\n",
    "print(lasso_coeffs)\n",
    "\n",
    "# number of zero coefficients\n",
    "zero_coeff = sum([beta == 0 for beta in lasso.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles & Pipelines\n",
    "Finally you'll learn how to make your models more efficient. You'll find out how to use ipelines to make your code clearer and easier to maintain. Then you'll use cross-validation to better test your models and dselct good model parameters. Finally you'll dabble in two types of ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Streamline workflow and ensure that training and testing data are treated consistently and no leakage of information between these two sets takes place.\n",
    "\n",
    "Leakage?\n",
    "- `.fit()` to training data **only**\n",
    "- `.transform` can be applied to both training and testing data\n",
    "\n",
    "A pipeline consists of a series of operations. You could apply each operation individually.. or you could just apply the pipeline!\n",
    "\n",
    "Cars model: Pipeline\n",
    "- `from pyspark.ml import Pipeline`\n",
    "- `pipeline = Pipeline(stages=[indexer, onehot, assemble, regression])`\n",
    "- `pipeline = pipeline.fit(cars_train)`\n",
    "- `predictions = pipeline.transform(cars_test)`\n",
    "\n",
    "You can access individual stages using the `.stages` attribute\n",
    "- `pipeline.stages[3] #fourth state = LinearRegression object`\n",
    "- `print(pipeline.stages[3].intercept)`\n",
    "- `print(pipeline.stages[3].coefficients)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fresh flights DataFrame\n",
    "flights = spark.read.csv('../data/flights.csv',\n",
    "                        sep=',',\n",
    "                        header=True,\n",
    "                        inferSchema=True,\n",
    "                        nullValue='NA')\n",
    "\n",
    "flights = flights.withColumn('km', round(flights.mile * 1.60934))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx', 'dow'], \n",
    "                                           outputCols=['org_dummy', 'dow_dummy'])\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'dow_dummy'],\n",
    "                           outputCol='features')\n",
    "                                           \n",
    "# a linear regression object\n",
    "regression = LinearRegression(labelCol='duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import class fo creating pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# construct a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "\n",
    "# train the pipeline on the training data\n",
    "pipeline = pipeline.fit(flights_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = pipeline.transform(flights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS spam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break text into tokens at non-word characters\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "# remove stop words\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\n",
    "\n",
    "# apply the hashing trick and transform to TF-IDF\n",
    "hasher = HashingTF(inputCol=remover.getOutputCol(), outputCol='hash')\n",
    "idf = IDF(inputCol=hasher.getOutputCol(), outputCol='features')\n",
    "\n",
    "# create a logistic regression object and add everything to a pipeline\n",
    "logistic = LogisticRegression()\n",
    "sms_pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "Grid and cross-validator\n",
    "- `from pyspark.ml.tuning import CrossValidator, ParamGridBuilder`\n",
    "- `params = ParamGridBuilder().build()`\n",
    "- `cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=10, seed=13)`\n",
    "- `cv = cv.fit(cars_train`\n",
    "- `cv.avgMetrics` average RMSE\n",
    "- `evaluator.evaluate(cv.transform(cars_test))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating simple flight duration\n",
    "In this exercise yoiu're going to train a simple model for flight duration using cross-validation. Travel time is usually strongly correlated with distance, so using the `km` column alone should give a decent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = VectorAssembler(inputCols=['km'], outputCol='features').transform(flights)\n",
    "\n",
    "flights_train, flights_test = flights.limit(1232).randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# create an empty parameter grid\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# create object for building and evaluating a regression model\n",
    "regression = LinearRegression(labelCol='duration')\n",
    "evaluator = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "# create a cross validator\n",
    "cv = CrossValidator(estimator=regression,\n",
    "                   estimatorParamMaps=params,\n",
    "                   evaluator=evaluator,\n",
    "                   numFolds=5,\n",
    "                   seed=13)\n",
    "\n",
    "# train and test model on multiple folds of the training data\n",
    "cv = cv.fit(flights_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating flight duration model pipeline\n",
    "In this exercise you'll add the `org` field to the model However, since `org` is categorical, there's more work to be done before itr can be included; it must first be transofrmed to an index and then one-hot encoded before being assembled with `km` and used to build the regression model. We'll wrap these operations up in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an indexer for the org field\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# create a one-hot encoder for the index org field\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# assemble the km and one-hot encoded fields\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "\n",
    "# create a pipeline and cross-validator\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                   estimatorParamMaps=params,\n",
    "                   evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "Parameter grid\n",
    "- `from pyspark.ml.tuning import ParamGridBuilder`\n",
    "- `params = ParamGridBuilder()`\n",
    "- `params = params.addGrid(regression.fitIntercept, [True, False])` add grid points\n",
    "- `params = params.build()` construct the grid\n",
    "\n",
    "Grid search with cross-validation\n",
    "- `cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator)`\n",
    "- `cv = cv.setNumFolds(10).setSeed(13).fit(cars_train)`\n",
    "<br>*since there are 2 points on the grid and 10 folds; this translates to 20 models*\n",
    "- `cv.avgMetrics`\n",
    "<br> > `[0.800663722151, 0.9079977823]` #in a list because you get one value for each point on the grid\n",
    "- `cv.bestModel` # access the best model\n",
    "- `predictions = cv.transform(cars_test)` # use the cross-validator object it will use the best model automatically\n",
    "- `cv.bestModel.explainParam('fitIntercept')` # retrieve the best parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing flights linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  16\n"
     ]
    }
   ],
   "source": [
    "# create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# add grids for two parameters\n",
    "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]).addGrid(regression.elasticNetParam, [0.0, 0.5, 1,0])\n",
    "\n",
    "# build the parameter grid\n",
    "params = params.build()\n",
    "print('Number of models to be tested: ', len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                   estimatorParamMaps=params,\n",
    "                   evaluator=evaluator,\n",
    "                   numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissecting the best flight duration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CrossValidator' object has no attribute 'bestModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d9b81ea7a02c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflights_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# get the best model from cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# look at the stages in the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrossValidator' object has no attribute 'bestModel'"
     ]
    }
   ],
   "source": [
    "flights_train, flights_test = flights.limit(1232).randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "cv = cv.fit(flights_train)\n",
    "\n",
    "# get the best model from cross validation\n",
    "best_model = cv.bestModel\n",
    "\n",
    "# look at the stages in the best model\n",
    "print(best_model.stages)\n",
    "\n",
    "# get the parameters fr the LinearRegression object in the best model\n",
    "best_model.stages[3].extractParamMap()\n",
    "\n",
    "# generate predicrtions on testing darta using the best model then calculate RMSE\n",
    "predictions = best_model.transform(flights_test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS spam optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# add grid for hashin trick parameters\n",
    "params = params.addGrid(hasher.numFeatures, [1024, 4096, 16384]).addGrid(hasher.binary, [True, False])\n",
    "\n",
    "# add grid for logistic regression parameters\n",
    "params = params.addGrid(logistic.regParam, [0.01, 0.1, 1.0, 10.0]).addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "\n",
    "params = params.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "- It's a collection of models\n",
    "- Wisdom of the Crowd - collective opinion of a group is better than that of a single expert\n",
    "> **Diversity** and **independence** are important because the best collective decisions are the product of disagreement and contest, not consensus or compromise. \n",
    "<br> - James Surowiecki, *The Wisdom of Crowds*\n",
    "\n",
    "Random Forest\n",
    "- works in parallel\n",
    "- an ensemble of Decision Trees\n",
    "- Creating model diversity:\n",
    "    - each tree trained on *random subset* of data\n",
    "    - *random subset* of features used for splitting at each node\n",
    "- No two trees in the forest should be the same\n",
    "- `from pyspark.ml.classification import RandomForestClassifier`\n",
    "- `forest = RandomForestClassifier(numTrees=5)`\n",
    "- `forest = forest.fit(cars_train)`\n",
    "- `forest.trees`\n",
    "- `forest.featureImportances`\n",
    "\n",
    "Gradient-Boosted Trees\n",
    "-  works in series\n",
    "- Iterative boosting algorithm:\n",
    "    1. Build a Decision Tree and add to ensemble\n",
    "    2. Predict label for each training instance using ensemble.\n",
    "    3. Compare predictions with known labels.\n",
    "    4. Emphasize training instances with incorrecrt predictions.\n",
    "    5. Return to 1.\n",
    "- Model improves each iteration\n",
    "- `from pyspark.ml.classification import GBTClassifier`\n",
    "- `gbt=GBTClassifier(maxIter=20)`\n",
    "- `gbt = gbt.fit(cars_train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed flights with Gradient-Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "flights_train, flights_test = flights_assembled.select('mon', 'depart', 'duration', 'features', 'label').randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeRegressionModel (uid=dtr_a825bf63fc04) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_86d1edf273db) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_9ccd639df39e) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_1759dcf98e9b) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_700af3baefa9) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_d36e2fa87dce) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_946ba3bba509) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_50be7f809177) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_419e2067639e) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_d79d007a77a7) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_116b9ecfc139) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_4ce1dbdce04f) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_e70f91999e4d) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_621d2aa8d1ba) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_421a5eff9fd7) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_3b5721109aa7) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_eb5e6789cd8a) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_81a7af44b94c) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_bfa5f9427740) of depth 5 with 63 nodes, DecisionTreeRegressionModel (uid=dtr_bb7b891d349f) of depth 5 with 63 nodes]\n",
      "(8,[0,1,2,3,4,5,6,7],[0.1938692556614394,0.15891716630793598,0.1519037095501831,0.09027852178376414,0.15344084353390006,0.06776195472218303,0.13542173422851103,0.04840681421208314])\n"
     ]
    }
   ],
   "source": [
    "# Import the classes required\n",
    "from pyspark.ml.classification import GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# create model objects and train on training data\n",
    "tree = DecisionTreeClassifier().fit(flights_train)\n",
    "gbt = GBTClassifier().fit(flights_train)\n",
    "\n",
    "# Compare AUC on testing data\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(tree.transform(flights_test))\n",
    "evaluator.evaluate(gbt.transform(flights_test))\n",
    "\n",
    "# find the number of trees and the relative iportance of features\n",
    "print(gbt.trees)\n",
    "print(gbt.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed flights with a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# create a random forest classifier\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# create a parameter grid\n",
    "params = ParamGridBuilder().addGrid(\n",
    "    forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']).addGrid(\n",
    "    forest.maxDepth, [2, 5, 10]).build()\n",
    "\n",
    "# create a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# create a cross-validator\n",
    "cv = CrossValidator(estimator=forest, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cv.fit(flights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average AUC for each paramter combination in grid\n",
    "avg_auc = cv.avgMetrics\n",
    "\n",
    "# average AUC for the best model\n",
    "best_model_auc = max(avg_auc)\n",
    "\n",
    "# whats the optiml parameter value?\n",
    "opt_max_depth = cv.bestModel.explainParam('maxDepth')\n",
    "opt_feat_substrat = cv.bestModel.explainParam('featureSubsetStrategy')\n",
    "\n",
    "# AUC for best model on testing data\n",
    "best_auc = evaluator.evaluate(cv.transform(flights_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto, current: all)\n",
      "0.7242267382150237\n"
     ]
    }
   ],
   "source": [
    "print(opt_max_depth)\n",
    "print(opt_feat_substrat)\n",
    "print(best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

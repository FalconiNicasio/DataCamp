{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Spark is a framework for working with Big Data. In this chapter you'll cover some background about SPark and Machine Learning. You'l then find out how to connect to Spark using Python and load CSV data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning & Spark\n",
    "\n",
    "Data in RAM\n",
    "- The performance on a ML model depends on data\n",
    "- If the data can't fit entirely in RAM then it will start to page data between RAM and Disk and the bigger the data the more time the computer spends waiting for data.\n",
    "- One option to solve is to distribute the data amoung clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Spark\n",
    "\n",
    "Spark is currently the most popular technology for processing large quantities of data. Not only is it able to handle enormous data volumes, but it does so very efficiently too! Also, unlike some other distributed computing technologies, developing with Spark is a pleasure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in a Spark Cluster\n",
    "Spark is a distributed computing platform. It achieves efficiency by distributing data and computation across a cluster of computers.\n",
    "A Spark cluster consists of a number of hardware and software components which work together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Spark\n",
    "Languages for interacting with Spark.\n",
    "- Java - low-level, compiled\n",
    "- Scala, Python, and R - high-level with interactive REPL (read.eval.print.loop)\n",
    "\n",
    "Importing pyspark\n",
    "- import `pyspark`\n",
    "\n",
    "Sub-modules\n",
    "- Structured Data - `pyspark.sql`\n",
    "- Streaming Data - `pyspark.streaming`\n",
    "- Machine Learning - `pyspark.mllib` (deprecated) and `pyspark.ml`\n",
    "\n",
    "Spark URL\n",
    "- **Remote Cluster** using Spark URL - spark://<IP address | DNS name>:<\\port>\n",
    "    - *example* spark://13.59.151.161:7077\n",
    "- **Local Cluster**\n",
    "    - `local` - only 1 core;\n",
    "    - ` local[4]` - 4 cores;\n",
    "    - `locall[*]` - all available cores.\n",
    "\n",
    "Creating a SparkSession\n",
    "- connect to spark by using:\n",
    "<br>```from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('first_spark_app').getOrCreate()```\n",
    "\n",
    "- close connection to spark\n",
    "<br>`spark.stop()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of Spark master\n",
    "The following are all examples of valid ways to specify the location of a Spark Cluster:\n",
    "- spark://13.59.151.161:7077\n",
    "- spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077\n",
    "- local\n",
    "- local[4]\n",
    "- local[*]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "# import the PySpark module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# what version of Spark?\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "`DataFrame` for tabular data.\n",
    "\n",
    "Reading data from CSV\n",
    "<br>&emsp;`cars = spark.read.csv('cars.csv', header=True)`\n",
    "\n",
    "- Optional arguments:\n",
    "    - `header` - is first row a header? (defaule:`False`)\n",
    "    - `sep` - field separator (default: a comma `','`)\n",
    "    - `schema` - explicit column data types\n",
    "    - `inferSchema` - deduce column data types from data?\n",
    "    - `nullValue` - placeholder for missing data\n",
    "    \n",
    "Peek at the data\n",
    "<br>&emsp;&emsp;`cars.show(5)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading flights data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 50000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mon', 'int'),\n",
       " ('dom', 'int'),\n",
       " ('dow', 'int'),\n",
       " ('carrier', 'string'),\n",
       " ('flight', 'int'),\n",
       " ('org', 'string'),\n",
       " ('mile', 'int'),\n",
       " ('depart', 'double'),\n",
       " ('duration', 'int'),\n",
       " ('delay', 'int')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from CSV file\n",
    "flights = spark.read.csv('../data/flights.csv',\n",
    "                        sep=',',\n",
    "                        header=True,\n",
    "                        inferSchema=True,\n",
    "                        nullValue='NA')\n",
    "\n",
    "# get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# view the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# check column data types\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SMS spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# load data from a delimited file\n",
    "sms = spark.read.csv(\"../data/sms.csv\", sep=';', header=False, schema=schema)\n",
    "\n",
    "# print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Now that you are familiar with getting data into Spark, you'll move onto buiilding two types of classification model: Decision Trees and Logistic Regression. You'll also find out about a few approaches to data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Dropping Colunns\n",
    "- Drop the columns you don't want\n",
    "<br>&emsp;`cars = cars.drop('maker', 'model')`\n",
    "\n",
    "- Select columns you want to retain\n",
    "<br>&emsp;`cars = cars.select('origin', 'type', 'cyl', 'size', 'weight', 'length', 'rpm', 'consumption')`\n",
    "\n",
    "Filtering out missing data\n",
    "- How many missing values?\n",
    "<br>&emsp;`cars.filter('cyl IS NULL').count()`\n",
    "\n",
    "- Drop records with missing values in the `cylinders` column\n",
    "<br>&emsp;`cars = cars.filter('cyl IS NOT NULL')`\n",
    "\n",
    "- Drop records with missing values in **any** column\n",
    "<br>&emsp;`cars = cars.dropna()`\n",
    "\n",
    "Mutating columns\n",
    "<br>&emsp;`from pyspark.sql.functions import round`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`cars = cars.withColumn('mass', round(cars.weight / 2.205, 0)`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`cars.withColumn('length', round(cars.length * 0.0254, 3))`\n",
    "\n",
    "Indexing categorical data\n",
    "<br>&emsp;`From pyspark.ml.feature import StringIndexer`\n",
    "<br>&emsp;\n",
    "<br>&emsp; `indexer = StringIndexer(inputCol='type', outputCol='type_idx')`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`indexer = indexer.fit(cars)`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`cars = indexer.transform(cars)`\n",
    "\n",
    "Assembling columns\n",
    "- consolidate input columns into a single column\n",
    "- ML algorithms in Spark operate on a single vector\n",
    "<br>&emsp;`from pyspark.ml.feature import VectorAssembler`\n",
    "<br>&emsp;\n",
    "<br>&emsp;`assembler = VectorAssembler(inputCols=['cyl','size'], outputCol='features')`\n",
    "<br>&emsp;`assembler.transform(cars)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n"
     ]
    }
   ],
   "source": [
    "# remove the 'flight' column\n",
    "flights = flights.drop('flight')\n",
    "\n",
    "# number of records with missing 'delay' values\n",
    "flights.filter('delay IS NULL').count()\n",
    "\n",
    "# remove records with missing 'delay' values\n",
    "flights = flights.filter('delay IS NOT NULL')\n",
    "\n",
    "# remove records with missing values in any column and get the number of remaining rows\n",
    "flights = flights.dropna()\n",
    "print(flights.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# convert 'mile' to 'km' and drop 'mile' column\n",
    "flights_km = flights.withColumn('km', round(flights.mile * 1.60934)).drop('mile')\n",
    "\n",
    "# create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# indexer identifies catgories in the data\n",
    "indexer_model = indexer.fit(flights_km)\n",
    "\n",
    "# indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights_km)\n",
    "\n",
    "# repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |30   |\n",
      "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |-8   |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|-5   |\n",
      "|[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]   |2    |\n",
      "|[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0] |54   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_indexed)\n",
    "\n",
    "# check the resulting columns\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Recursive partitioning\n",
    "\n",
    "Stopping criteria examples (stop splitting)\n",
    "- number of records of a node fall below a threshold\n",
    "- purity of a node is above a threshold\n",
    "\n",
    "Split train/test\n",
    "- `train, test = cars.randomSplit([0.8, 0.2], seed=23)`\n",
    "\n",
    "Build a decision tree\n",
    "- `from pyspark.ml.classification import DecisionTreeClassifier`\n",
    "- `tree = DecisionTreeClassifier()`\n",
    "- `tree_model = tree.fit(train)`\n",
    "- `prediction = tree.transform(test)`\n",
    "\n",
    "Confusion matrix\n",
    "- A confusion matrix is a table which describes performance of a model on testing data.\n",
    "- `prediction.groupBy(\"label\", \"prediction\").count().show()`\n",
    "- *note Accuracy = (TN + TP)/(TN + TP + FN + FP) - proportion of correct predictions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980732423121092\n"
     ]
    }
   ],
   "source": [
    "# split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# check that training set has around 80% of recods\n",
    "training_ratio = flights_train.count()/flights.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------------------------------+\n",
      "|label|prediction|probability                            |\n",
      "+-----+----------+---------------------------------------+\n",
      "|1    |1.0       |[0.4911242603550296,0.5088757396449705]|\n",
      "|1    |1.0       |[0.3543011744450593,0.6456988255549406]|\n",
      "|1    |1.0       |[0.3543011744450593,0.6456988255549406]|\n",
      "|1    |1.0       |[0.3543011744450593,0.6456988255549406]|\n",
      "|1    |1.0       |[0.3543011744450593,0.6456988255549406]|\n",
      "+-----+----------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|            features|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|[0.0,22.0,2.0,0.0...|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|[2.0,20.0,4.0,0.0...|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|[9.0,13.0,1.0,1.0...|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|[5.0,2.0,1.0,0.0,...|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|[7.0,2.0,6.0,1.0,...|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_assembled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1089|\n",
      "|    0|       0.0| 2283|\n",
      "|    1|       1.0| 3735|\n",
      "|    0|       1.0| 2388|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0.6338072669826225\n"
     ]
    }
   ],
   "source": [
    "# create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# accuracy measure the propeortion of correct predictions\n",
    "accuracy = (TN + TP)/(TN + TP + FN + FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- Uses a logistic function to measure a binary target (0,1 : true, false)\n",
    "- X-axis is a linear combination of predictor variables\n",
    "- y-axis output of the model\n",
    "- the model dreives coefficients for each numerical predictor\n",
    "    - can shift the curve to the right or left\n",
    "    - can make the transition more gradual or steep\n",
    "    \n",
    "Build a Logistic Regression model\n",
    "- `from pyspark.ml.classification import LogisticRegression`\n",
    "- `logistic = LogisticRegression()`\n",
    "- `logistic_model = logistic.fit(train)`\n",
    "- `prediction = logistic_model.transform(test)`\n",
    "\n",
    "Precision and recall\n",
    "- build confusion matrix\n",
    "- precision = TP / (TP + FP) *note proportion of positive predictions*\n",
    "- recall = TP / (TP + FN) *note proportion of positive targets that are correctly predicted*\n",
    "\n",
    "Weighted metrics\n",
    "- `from pyspark.ml.evaluation import MulticlassClassificationEvaluator`\n",
    "- `evaluator = MulticlassClassificationEvaluator()`\n",
    "- `evaluator.evaluate(prediction, {evaluator.metricName: 'weightedPrecision'})`\n",
    "    - Other metrics: `weightedRecall`, `accuracy`, `f1`\n",
    "    \n",
    "ROC and AUC\n",
    "ROC = \"Receiver Operating Characteristic\"\n",
    "- TP versus FP\n",
    "- threshold = 0 (top right)\n",
    "- threshdol = 1 (bottom left)\n",
    "AUC = \"Area under the curve\"\n",
    "- ideally AUC = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1666|\n",
      "|    0|       0.0| 2636|\n",
      "|    1|       1.0| 3169|\n",
      "|    0|       1.0| 2024|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_num = flights_assembled.select('mon', 'depart', 'duration', 'features', 'label')\n",
    "flights_num_train, flights_num_test = flights_num.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# create a classifier object and train on the training data\n",
    "logistic = LogisticRegression().fit(flights_num_train)\n",
    "\n",
    "# create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_num_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.61\n",
      "recall = 0.66\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction,\n",
    "                                              {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Text into Tables\n",
    "\n",
    "Removing punctuation\n",
    "- `from pyspark.sql.functions import regexp_replace`\n",
    "- `REGEX = '[,\\\\-]'`\n",
    "- `books = books.withColumn('text', regexp_replace(books.text, REGEX, ' '))`\n",
    "\n",
    "Text to tokens\n",
    "- `from pyspark.ml.feature import Tokenizer`\n",
    "- `books = Tokenizer(inputCol=\"text\", outputCol=\"tokens\").transform(books)`\n",
    "\n",
    "What are stop words\"\n",
    "- `from pyspark.ml.feature import StopWordsRemover`\n",
    "- `stopwords = StopWordsRemover()`\n",
    "- `stopwords.getStopWords()`\n",
    "\n",
    "Removing stop words\n",
    "- `stopwords = stopwords.setInputCol('tokens').setOutputCol('words')`\n",
    "- `books = stopwords.transform(books)`\n",
    "\n",
    "Feature hashing\n",
    "- `from pyspark.ml.feature import HashingTF`\n",
    "- `hasher = HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=32)`\n",
    "- `books = hasher.transform(books)`\n",
    "\n",
    "Dealing with common words\n",
    "- IDF = inverse documents frequency\n",
    "- `from pyspark.ml feature import IDF`\n",
    "- `books = IDF(inputCol=\"hash\", outputCol=\"features\").fit(books).transform(books)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation, numbers and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|id |text                              |label|words                                     |\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# remove punctuation\n",
    "sms = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "sms = sms.withColumn('text', regexp_replace(sms.text, '[0-9]', ' '))\n",
    "\n",
    "# merge multiple spaces\n",
    "sms = sms.withColumn('text', regexp_replace(sms.text, ' +', ' '))\n",
    "\n",
    "# split the text into words\n",
    "sms = Tokenizer(inputCol='text', outputCol='words').transform(sms)\n",
    "\n",
    "sms.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words and hashing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|terms                           |features                                                                                            |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]   |(1024,[138,344,378,1006],[2.2391682769656747,2.892706319430574,3.684405173719015,4.244020961654438])|\n",
      "|[dont, worry, guess, busy]      |(1024,[53,233,329,858],[4.618714411095849,3.557143394108088,4.618714411095849,4.937168142214383])   |\n",
      "|[call, freephone]               |(1024,[138,396],[2.2391682769656747,3.3843005812686773])                                            |\n",
      "|[win, cash, prize, prize, worth]|(1024,[31,69,387,428],[3.7897656893768414,7.284881949239966,4.4671645129686475,3.898659777615979])  |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# remove stop words\n",
    "sms = StopWordsRemover(inputCol=\"words\", outputCol=\"terms\").transform(sms)\n",
    "\n",
    "# apply the hashing trick\n",
    "sms = HashingTF(inputCol=\"terms\", outputCol=\"hash\", numFeatures=1024).transform(sms)\n",
    "\n",
    "# convert hashed symbols to TF-IDF\n",
    "sms = IDF(inputCol=\"hash\", outputCol=\"features\").fit(sms).transform(sms)\n",
    "\n",
    "sms.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a spam classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1666|\n",
      "|    0|       0.0| 2636|\n",
      "|    1|       1.0| 3169|\n",
      "|    0|       1.0| 2024|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing sets\n",
    "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# fit a logistic regresssion model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "predictions = logistic.transform(sms_test)\n",
    "\n",
    "# create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Next you'll learn to create Linear Regression models. You'll also find out how to augment your data by engineering new predictors as well as a robiust approach to selecting ony the most relevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Dummy variables\n",
    "- Each categorical level becomes a column\n",
    "- Binary values indicate the presence (1) or absence (0) of the corresponding level\n",
    "- Sparse reprersentation: store column index and value\n",
    "\n",
    "One-hot encoding\n",
    "- `from pyspark.ml.feature import OneHotEncoderEstimator`\n",
    "- `onehot = OneHotEncoderEstimator(imputCols=['type_idx'], outputCols=['type_dummy'])`\n",
    "- `onehot = onehot.fit(cars)`\n",
    "- `onehot.categorySizes`\n",
    "- `cars = onehot.transform(cars)`\n",
    "- `cars.select('type, 'type_idx', 'type_dummy').distinct().sort('type_idx').show()`\n",
    "\n",
    "Dense versus sparse\n",
    "- `from pyspark.mllib.linalg import DenseVector, SparseVector`\n",
    "- Store this vector:[1, 0, 0, 0, 0, 7, 0, 0])\n",
    "- `DenseVector([1, 0, 0, 0, 0, 7, 0, 0])`\n",
    "    - `out:` DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0])\n",
    "- `SparseVector(8, [0, 5], [1, 7])`\n",
    "    - `out:` SparseVector(8, {0: 1.0, 5: 7.0})\n",
    "- SparseVector(length of the vector, index of non-zero values, non-zero values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset flights dataframe\n",
    "flights = spark.read.csv('../data/flights.csv',\n",
    "                        sep=',',\n",
    "                        header=True,\n",
    "                        inferSchema=True,\n",
    "                        nullValue='NA')\n",
    "\n",
    "flights = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights).transform(flights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding flight origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SJC|    4.0|(7,[4],[1.0])|\n",
      "|SMF|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights = onehot.transform(flights)\n",
    "\n",
    "# check the results\n",
    "flights.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enocidng shirt sizes\n",
    "You have data for a consignment of t-shirts. The data includes th size of the shirt, which is given as either S,M, L, or XL.\n",
    "\n",
    "Here Are the counts for the different sizes.\n",
    "\n",
    "`+----+-----+\n",
    "|size|count|\n",
    "+----+-----+\n",
    "|   S|    8|\n",
    "|   M|   15|\n",
    "|   L|   20|\n",
    "|  XL|    7|\n",
    "+----+-----+`\n",
    "\n",
    "The sizes are first converted to an index using `StringIndexer` and then one-hot encoded using `OneHotEncorderEstimator`\n",
    "\n",
    "Output:\n",
    "\n",
    "`+---+-------+-------------+\n",
    "|size|   idx|    idx_dummy|\n",
    "+---+-------+-------------+\n",
    "|  S|    2.0|(3,[2],[1.0])|\n",
    "|  M|    1.0|(3,[1],[1.0])|\n",
    "|  L|    0.0|(3,[0],[1.0])|\n",
    "| XL|    3.0|(3,[3],  [] )|\n",
    "+---+-------+-------------+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "Mininmizing Loss functiopn\n",
    "- MSE = \"Mean Squared Error\"\n",
    "\n",
    "Assemble predictors\n",
    "- Predict `consumptiopn` using `mass`,`cyl`,and `type_dummy`.\n",
    "- Consolidate predictors into a single column called `features`\n",
    "\n",
    "Build regression model\n",
    "- `from pyspark.ml.regression import LinearRegression`\n",
    "- `regression = LinearRegression(labelCol='consumption')`\n",
    "- `regression = regression.fit(cars_train)`\n",
    "- `predictions = regression.transform(cars_test)`\n",
    "\n",
    "Calculate RMSE\n",
    "- `from pyspark.ml.evaluation import RegressionEvaluator`\n",
    "- `RegressionEvaluator(labelCol='consumption').evaluate(predictions)`\n",
    "- `RegressionEvaluator` can also calculate the following metrics\n",
    "    - `mae` (Mean Absolute Error)\n",
    "    - `r2` (R^2)\n",
    "    - `mse` (Mean Squared Error)\n",
    "\n",
    "Examine intercept\n",
    "- `regression.intercept`\n",
    "- This is the fuel consumption in the (hypothetical) case that:\n",
    "    - `mass` = 0\n",
    "    - `cyl` = 0\n",
    "\n",
    "Examine Coefficients\n",
    "- `regression.coefficients`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'mile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a34fd30d51e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'km'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmile\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.60934\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'km'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1300\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'mile'"
     ]
    }
   ],
   "source": [
    "flights = flights.withColumn('km', flights.mile * 1.60934).drop('mile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['km'], outputCol='features')\n",
    "flights = assembler.transform(flights)\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+----------+------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    org_dummy|        km|    features|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+----------+------------+\n",
      "| 11| 20|  6|     US|    19|JFK|  9.48|     351| null|    2.0|(7,[2],[1.0])|3464.90902|[3464.90902]|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30|    0.0|(7,[0],[1.0])| 508.55144| [508.55144]|\n",
      "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8|    1.0|(7,[1],[1.0])| 542.34758| [542.34758]|\n",
      "|  9| 13|  1|     AA|   419|ORD| 10.33|     195|   -5|    0.0|(7,[0],[1.0])|1989.14424|[1989.14424]|\n",
      "|  4|  2|  5|     AA|   325|ORD|  8.92|      65| null|    0.0|(7,[0],[1.0])| 415.20972| [415.20972]|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Just distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|385     |359.2696656028616 |\n",
      "|135     |149.93838859748362|\n",
      "|200     |224.09938202172748|\n",
      "|64      |72.97656947741446 |\n",
      "|259     |269.1561432154389 |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.039835602281954"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.35943736789506\n",
      "[0.07566768380408988]\n",
      "792.9408828654666\n"
     ]
    }
   ],
   "source": [
    "# intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# average minutes per km\n",
    "minutes_per_km = coefs[0]\n",
    "\n",
    "# average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Adding origin airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mon',\n",
       " 'dom',\n",
       " 'dow',\n",
       " 'carrier',\n",
       " 'flight',\n",
       " 'org',\n",
       " 'depart',\n",
       " 'duration',\n",
       " 'delay',\n",
       " 'org_idx',\n",
       " 'org_dummy',\n",
       " 'km',\n",
       " 'features']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.drop('features')\n",
    "\n",
    "# add org_dummy to features\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "flights = assembler.transform(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.124011989821785"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting coefficients\n",
    "\n",
    "The values for km and org_dummy have been assembled into features, which has eight columns with sparse representation. Column indices in features are as follows:\n",
    "\n",
    "- 0 — km\n",
    "- 1 — ORD\n",
    "- 2 — SFO\n",
    "- 3 — JFK\n",
    "- 4 — LGA\n",
    "- 5 — SMF\n",
    "- 6 — SJC and\n",
    "- 7 — TUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807.3233680861835\n",
      "15.862995338536262\n",
      "68.52994024026248\n",
      "62.568008835633506\n"
     ]
    }
   ],
   "source": [
    "# average speed in km per hour\n",
    "avg_speed_hour = 60 / regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0743, 28.3907, 20.54, 52.6669, 46.705, 18.2755, 15.7032, 17.7257])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

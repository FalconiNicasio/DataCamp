{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark SQL with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark SQL\n",
    "\n",
    "In this chapter you will learn how to create and query a SQL table in Spark. Spark SQL brings the expressiveness of SQL to Spark. You will also learn how to use SQL window functions in Spark. Window functions perform a calculation across rows taht are related to the current row. They greatly simplify achieving results that are difficult to express using only joins and traditional aggregations. We'll use window functions to perform running sums, running differences, and other operatios that are challenging to perform in basic SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQL table from a dataframe\n",
    "\n",
    "A dataframe can be used to create a **temporary table**. A *temporary table* is one that will not exist after the session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainsched.txt\n",
    "df = spark.read.csv(\"../data/trainsched.txt\", header = True)\n",
    "\n",
    "# create temporary tabl called schedule\n",
    "df.createOrReplaceTempView(\"schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the column names of a table\n",
    "\n",
    "After creating a DataFrame you can query the data using SQL statements\n",
    "> spark.sql(\"SELECT * FROM schedule WHERE station = 'San Jose'\").show()\n",
    "\n",
    "> result = spark.sql(\"SHOW COLUMNS FROM tablename\")\n",
    "<br>result = spark.swl(\"SELECT * FROM tablename LIMIT 0\")\n",
    "<br>result = spark.sql(\"DESCRIBE tablename\")\n",
    "<br>result.show()\n",
    "<br>print(results.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|train_id|   string|   null|\n",
      "| station|   string|   null|\n",
      "|    time|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspecrt the columns in the table df\n",
    "spark.sql(\"DESCRIBE schedule\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Window Function SQL?\n",
    "- Express operations more simply than dot notation or queries\n",
    "- Each row uses the values of other rows to calculate its value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot notation and SQL\n",
    "\n",
    "Pretty much a dot notation for every SQL clause, even window functions. For example:\n",
    "> from pyspark.sql import Window\n",
    "<br>from pyspark.sql.functions import row_number\n",
    "<br>\n",
    "<br>df.withColumn(\"id\", row_number().over(Window.partitionBy('train_id').orderBy('time')))\n",
    "\n",
    "Is the same as\n",
    ">query = \"\"\"\n",
    "<br>SELECT *\n",
    "<br>ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id\n",
    "<br>FROM schedule\n",
    "\"\"\"\n",
    "<br>\n",
    "<br>spark.sql(query).show(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation, step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_id: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+---------+---------+\n",
      "|train_id|min(time)|max(time)|\n",
      "+--------+---------+---------+\n",
      "|     217|    6:06a|    6:59a|\n",
      "|     324|    7:59a|    9:05a|\n",
      "+--------+---------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|train_id|max(time)|\n",
      "+--------+---------+\n",
      "|     217|    6:59a|\n",
      "|     324|    9:05a|\n",
      "+--------+---------+\n",
      "\n",
      "max(time)\n"
     ]
    }
   ],
   "source": [
    "# give the identical result in each command\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\n",
    "\n",
    "df.groupBy(df.train_id).agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n",
    "# print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\n",
    "\n",
    "result = df.groupBy('train_id').agg({'time':'min','time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating the same column twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n",
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, col\n",
    "expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\n",
    "dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
    "dot_df.show()\n",
    "\n",
    "# Write a SQL query giving a result identical to dot_df\n",
    "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate dot SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+---------+\n",
      "|train_id|      station| time|time_next|\n",
      "+--------+-------------+-----+---------+\n",
      "|     217|       Gilroy|6:06a|    6:15a|\n",
      "|     217|   San Martin|6:15a|    6:21a|\n",
      "|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|     217|      Capitol|6:42a|    6:50a|\n",
      "|     217|       Tamien|6:50a|    6:59a|\n",
      "|     217|     San Jose|6:59a|     null|\n",
      "|     324|San Francisco|7:59a|    8:03a|\n",
      "|     324|  22nd Street|8:03a|    8:16a|\n",
      "|     324|     Millbrae|8:16a|    8:24a|\n",
      "|     324|    Hillsdale|8:24a|    8:31a|\n",
      "|     324| Redwood City|8:31a|    8:37a|\n",
      "|     324|    Palo Alto|8:37a|    9:05a|\n",
      "|     324|     San Jose|9:05a|     null|\n",
      "+--------+-------------+-----+---------+\n",
      "\n",
      "+--------+-------------+-----+---------+\n",
      "|train_id|      station| time|time_next|\n",
      "+--------+-------------+-----+---------+\n",
      "|     217|       Gilroy|6:06a|    6:15a|\n",
      "|     217|   San Martin|6:15a|    6:21a|\n",
      "|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|     217|      Capitol|6:42a|    6:50a|\n",
      "|     217|       Tamien|6:50a|    6:59a|\n",
      "|     217|     San Jose|6:59a|     null|\n",
      "|     324|San Francisco|7:59a|    8:03a|\n",
      "|     324|  22nd Street|8:03a|    8:16a|\n",
      "|     324|     Millbrae|8:16a|    8:24a|\n",
      "|     324|    Hillsdale|8:24a|    8:31a|\n",
      "|     324| Redwood City|8:31a|    8:37a|\n",
      "|     324|    Palo Alto|8:37a|    9:05a|\n",
      "|     324|     San Jose|9:05a|     null|\n",
      "+--------+-------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_next = spark.sql(\"\"\"\n",
    "SELECT *, \n",
    "LEAD(time,1) OVER(PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\")\n",
    "df_time_next.show()\n",
    "\n",
    "# obtain the identical result using dot notation\n",
    "from pyspark.sql.functions import lead\n",
    "from pyspark.sql import Window\n",
    "\n",
    "dot_df = df.withColumn('time_next', lead('time', 1)\n",
    "                       .over(Window.partitionBy('train_id')\n",
    "                            .orderBy('time')))\n",
    "dot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert window function from dot notation to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+\n",
      "|train_id|      station| time|diff_min|\n",
      "+--------+-------------+-----+--------+\n",
      "|     217|       Gilroy|6:06a|     9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|\n",
      "|     217|      Capitol|6:42a|     8.0|\n",
      "|     217|       Tamien|6:50a|     9.0|\n",
      "|     217|     San Jose|6:59a|    null|\n",
      "|     324|San Francisco|7:59a|     4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|\n",
      "|     324| Redwood City|8:31a|     6.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|\n",
      "|     324|     San Jose|9:05a|    null|\n",
      "+--------+-------------+-----+--------+\n",
      "\n",
      "+--------+-------------+-----+--------+\n",
      "|train_id|      station| time|diff_min|\n",
      "+--------+-------------+-----+--------+\n",
      "|     217|       Gilroy|6:06a|     9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|\n",
      "|     217|      Capitol|6:42a|     8.0|\n",
      "|     217|       Tamien|6:50a|     9.0|\n",
      "|     217|     San Jose|6:59a|    null|\n",
      "|     324|San Francisco|7:59a|     4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|\n",
      "|     324| Redwood City|8:31a|     6.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|\n",
      "|     324|     San Jose|9:05a|    null|\n",
      "+--------+-------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "window = Window.partitionBy('train_id').orderBy('time')\n",
    "dot_df = df.withColumn('diff_min', \n",
    "                    (unix_timestamp(lead('time', 1).over(window),'H:m') \n",
    "                     - unix_timestamp('time', 'H:m'))/60)\n",
    "dot_df.show()\n",
    "\n",
    "# create a SQL query to obtain an identical result to dot_df\n",
    "query = \"\"\"\n",
    "SELECT *,\n",
    "(UNIX_TIMESTAMP(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time), 'H:m')\n",
    "- UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()\n",
    "\n",
    "sql_df.createOrReplaceTempView(\"schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running sums using window function SQL\n",
    "\n",
    "A window function is like an aggregate function, except that it gives an output for every row in the dataset instead of a single row per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+-------------+\n",
      "|train_id|      station| time|diff_min|running_total|\n",
      "+--------+-------------+-----+--------+-------------+\n",
      "|     217|       Gilroy|6:06a|     9.0|          9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|         15.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|         30.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|         36.0|\n",
      "|     217|      Capitol|6:42a|     8.0|         44.0|\n",
      "|     217|       Tamien|6:50a|     9.0|         53.0|\n",
      "|     217|     San Jose|6:59a|    null|         53.0|\n",
      "|     324|San Francisco|7:59a|     4.0|          4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|         17.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|         25.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|         32.0|\n",
      "|     324| Redwood City|8:31a|     6.0|         38.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|         66.0|\n",
      "|     324|     San Jose|9:05a|    null|         66.0|\n",
      "+--------+-------------+-----+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time, diff_min,\n",
    "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using window function sql for natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading natural language text\n",
    "\n",
    "The dataset\n",
    "- **The Project Gutenberg eBook of The Adventures of Sherlock Holmes.** by Sir Arthur Conan Doyle.\n",
    "-Available from gutenberg.org\n",
    "\n",
    "Loading text use `spark.read.text`\n",
    "\n",
    "Loading parquet\n",
    "- use `spark.read.load`\n",
    "- parquet is **Hadoop**'s file format to store data stuctures\n",
    "\n",
    "Loaded text\n",
    "- `.show()` *arg truncate=False allows it to print longer rows\n",
    "- 'lower(col('value'))' converts column to lowercase\n",
    "- `.alias` method changes column name\n",
    "- `regexp_replace(colname, pattern to be replace, replacing text)` replaces text\n",
    "- `split()` tokenize text\n",
    "- `explode()` puts each word on its own\n",
    "- `monotonically_increasing_id()` adds row id column\n",
    "\n",
    "Partitioning data\n",
    "- can use `when()` and `otherwise()` case statement to partition the data\n",
    "- example\n",
    "    - > df2 = df.withColumn('title', when(df.id < 25000, 'Preface').when(df.id < 50000, 'Chapter 1').when(df.id < 75000, 'Chapter 2').otherwise('Chapter 3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a dataframe from a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|word  |id |\n",
      "+------+---+\n",
      "|it    |71 |\n",
      "|do    |72 |\n",
      "|not   |73 |\n",
      "|change|74 |\n",
      "|or    |75 |\n",
      "+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the dataframe\n",
    "df = spark.read.load(\"../data/sherlock.parquet\")\n",
    "\n",
    "# filter and show the first 5 rows\n",
    "df.filter('id > 70').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and explode a text column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, length, monotonically_increasing_id, split, explode\n",
    "\n",
    "clause_df = spark.read.text(\"../data/clause.txt\")\n",
    "clause_df = clause_df.select(lower(col('value')).alias('clause'))\n",
    "clause_df = clause_df.filter(length('clause') > 0)\n",
    "clause_df = clause_df.select(\"clause\", monotonically_increasing_id().alias(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|words                                            |\n",
      "+-------------------------------------------------+\n",
      "|[title:, the, adventures, of, sherlock, holmes]  |\n",
      "|[author:, sir, arthur, conan, doyle]             |\n",
      "|[release, date:, march,, 1999, , [ebook, #1661]] |\n",
      "|[[most, recently, updated:, november, 29,, 2002]]|\n",
      "|[edition:, 12]                                   |\n",
      "+-------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|    title:|\n",
      "|       the|\n",
      "|adventures|\n",
      "|        of|\n",
      "|  sherlock|\n",
      "|    holmes|\n",
      "|   author:|\n",
      "|       sir|\n",
      "|    arthur|\n",
      "|     conan|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Number of rows:  1260\n"
     ]
    }
   ],
   "source": [
    "# split the cluase column into a column called words\n",
    "split_df = clause_df.select(split('clause', ' ').alias('words'))\n",
    "split_df.show(5, truncate=False)\n",
    "\n",
    "# explode the words column into a column called word\n",
    "exploded_df = split_df.select(explode('words').alias('word'))\n",
    "exploded_df.show(10)\n",
    "\n",
    "# count the resulting number of rows in exploded_df\n",
    "print(\"\\nNumber of rows: \", exploded_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving window analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark SQL with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark SQL\n",
    "\n",
    "In this chapter you will learn how to create and query a SQL table in Spark. Spark SQL brings the expressiveness of SQL to Spark. You will also learn how to use SQL window functions in Spark. Window functions perform a calculation across rows taht are related to the current row. They greatly simplify achieving results that are difficult to express using only joins and traditional aggregations. We'll use window functions to perform running sums, running differences, and other operatios that are challenging to perform in basic SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQL table from a dataframe\n",
    "\n",
    "A dataframe can be used to create a **temporary table**. A *temporary table* is one that will not exist after the session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainsched.txt\n",
    "df = spark.read.csv(\"../data/trainsched.txt\", header = True)\n",
    "\n",
    "# create temporary tabl called table1\n",
    "df.createOrReplaceTempView(\"schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the column names of a table\n",
    "\n",
    "After creating a DataFrame you can query the data using SQL statements\n",
    "> spark.sql(\"SELECT * FROM schedule WHERE station = 'San Jose'\").show()\n",
    "\n",
    "> result = spark.sql(\"SHOW COLUMNS FROM tablename\")\n",
    "<br>result = spark.swl(\"SELECT * FROM tablename LIMIT 0\")\n",
    "<br>result = spark.sql(\"DESCRIBE tablename\")\n",
    "<br>result.show()\n",
    "<br>print(results.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|train_id|   string|   null|\n",
      "| station|   string|   null|\n",
      "|    time|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE schedule\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Window Function SQL?\n",
    "- Express operations more simply than dot notation or queries\n",
    "- Each row uses the values of other rows to calculate its value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running sums using window function SQL\n",
    "\n",
    "A window function is like an aggregate function, except that it gives an output for every row in the dataset instead of a singl row per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_id: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+---------+\n",
      "|train_id|      station| time|time_next|\n",
      "+--------+-------------+-----+---------+\n",
      "|     217|       Gilroy|6:06a|    6:15a|\n",
      "|     217|   San Martin|6:15a|    6:21a|\n",
      "|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|     217|      Capitol|6:42a|    6:50a|\n",
      "|     217|       Tamien|6:50a|    6:59a|\n",
      "|     217|     San Jose|6:59a|     null|\n",
      "|     324|San Francisco|7:59a|    8:03a|\n",
      "|     324|  22nd Street|8:03a|    8:16a|\n",
      "|     324|     Millbrae|8:16a|    8:24a|\n",
      "|     324|    Hillsdale|8:24a|    8:31a|\n",
      "|     324| Redwood City|8:31a|    8:37a|\n",
      "|     324|    Palo Alto|8:37a|    9:05a|\n",
      "|     324|     San Jose|9:05a|     null|\n",
      "+--------+-------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time) AS time_next\n",
    "FROM schedule\"\"\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot notation and SQL\n",
    "\n",
    "Pretty much a dot notation for every SQL clause, even window functions. For example:\n",
    "> from pyspark.sql import Window\n",
    "<br>from pyspark.sql.functions import row_number\n",
    "<br>\n",
    "<br>df.withColumn(\"id\", row_number().over(Window.partitionBy('train_id').orderBy('time')))\n",
    "\n",
    "Is the same as\n",
    ">query = \"\"\"\n",
    "<br>SELECT *\n",
    "<br>ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id\n",
    "<br>FROM schedule\n",
    "\"\"\"\n",
    "<br>\n",
    "<br>spark.sql(query).show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
